cluster:
  num_nodes: 1                                  # how many machines to ask Google for
  machine_type: n1-standard-8                   # type of each machine https://cloud.google.com/compute/docs/machine-types
  disk_size: 50                                 # local volume size (GB) available on each machine
  zone: europe-west3-b                          # one of https://cloud.google.com/compute/docs/regions-zones/
  autoscaling: False                            # whether to spin up/down nodes as number of containers is changed
  min_nodes: 1                                  # if autoscaling, never have fewer than this many machines
  max_nodes: 1                                  # if autoscaling, never have more than this many machines
  preemptible: False                            # cheaper nodes that may be killed at any time
jupyter:
  memory: 256Mi                                 # memory allocated to the Dockerfile.jupyter container, including all kernels
  cpus: 0.5                                     # cores used by the Dockerfile.jupyter container, including kernels
  port: 8888                                    # external HTTP access port for Dockerfile.jupyter notebook
  lab_port: 8889                                # external HTTP access port for jupyterlab interface
  image: drtools/dask:latest                    # docker image of the Dockerfile.jupyter environment
scheduler:
  memory: 256Mi                                 # memory allocated for the dask scheduler container
  cpus:  0.5                                    # cores used for the scheduler (which is a single-threaded process)
  image: drtools/dask:latest                    # docker image of the dask scheduler's environment
  tcp_port: 8786                                # external access port for the scheduler, through which clients connect
  http_port: 9786                               # external HTTP port for REST/JSON information from the scheduler
  bokeh_port: 8787                              # external HTTP port for the diagnostics dashboards
workers:
  count: 1                                      # number of worker containers to launch, with one worker process per container
  cpus_per_worker: 0.5                          # cores allocated per worker container; the number of threads per worker will
                                                # be rounded up to an integer
  memory_per_worker: 512Mi                      # memory of each worker container
  mem_factor: 0.95                              # memory limit option to pass to worker will be this multiplied by the container limit;
                                                # slightly less than one, so worker should not exceed available memory in the container
  image: drtools/dask:latest                    # docker image of each worker's environment